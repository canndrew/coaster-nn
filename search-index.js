var searchIndex = {};
searchIndex["coaster_nn"] = {"doc":"Provides a [Coaster][coaster] Plugin, to extend Coaster with Neural Network related operations such as convolutions, pooling, ReLU, etc. A full list of operations provided by this Plugin, can be found at the provided Operations section.","items":[[4,"ConvForwardAlgo","coaster_nn","Different algorithms to compute the convolution forward algorithm.",null,null],[13,"Auto","","Attempt to automatically find the best algorithm of all the other available ones.",0,null],[13,"GEMM","","Compute the convolution as explicit matrix product.",0,null],[13,"ImplicitGEMM","","Compute the convolution as matrix product without forming the matrix that holds the input data.",0,null],[13,"ImplicitPrecompiledGEMM","","Similar to `ImplicitGEMM` but needs some workspace to precompile the implicit indices.",0,null],[13,"FFT","","Compute the convolution as Fast-Fourier Transform.",0,null],[13,"FFTTiling","","Compute the convolution as Fast-Fourier Transform with 32x32 tiles.",0,null],[13,"Direct","","Compute the convolution without implicit or explicit matrix-multiplication. Do not try to use this.",0,null],[13,"Winograd","","Winograd  Transform",0,null],[13,"WinogradNonFused","","Winograd  Transform Non-Fused",0,null],[4,"ConvBackwardFilterAlgo","","Different algorithms to compute the gradient with respect to the filter.",null,null],[13,"Auto","","Attempt to automatically find the best algorithm of all the other available ones.",1,null],[13,"ImplicitGEMM","","Compute the convolution as matrix product without forming the matrix that holds the input data.",1,null],[13,"ImplicitGEMMSum","","Compute the convolution as sum of matrix product without forming the matrix that holds the input data.",1,null],[13,"ImplicitPrecompiledGEMMSum","","Similar to `ImplicitGEMMSum` but needs some workspace to precompile the implicit indices.",1,null],[13,"FFT","","Compute the convolution as Fast-Fourier Transform.",1,null],[13,"WinogradNonFused","","Winograd  Transform Non-Fused",1,null],[4,"ConvBackwardDataAlgo","","Different algorithms to compute the gradient with respect to the filter.",null,null],[13,"Auto","","Attempt to automatically find the best algorithm of all the other available ones.",2,null],[13,"ImplicitGEMM","","Compute the convolution as matrix product without forming the matrix that holds the input data.",2,null],[13,"ImplicitGEMMSum","","Compute the convolution as sum of matrix product without forming the matrix that holds the input data.",2,null],[13,"FFT","","Compute the convolution as Fast-Fourier Transform.",2,null],[13,"FFTTiling","","Compute the convolution as Fast-Fourier Transform with 32x32 tiles.",2,null],[13,"Winograd","","Winograd  Transform",2,null],[13,"WinogradNonFused","","Winograd  Transform Non-Fused",2,null],[11,"fmt","","",0,{"inputs":[{"name":"self"},{"name":"formatter"}],"output":{"name":"result"}}],[11,"clone","","",0,{"inputs":[{"name":"self"}],"output":{"name":"convforwardalgo"}}],[11,"is_auto","","Check if algorithim should be chosen automatically.",0,{"inputs":[{"name":"self"}],"output":{"name":"bool"}}],[11,"fmt","","",1,{"inputs":[{"name":"self"},{"name":"formatter"}],"output":{"name":"result"}}],[11,"clone","","",1,{"inputs":[{"name":"self"}],"output":{"name":"convbackwardfilteralgo"}}],[11,"is_auto","","Check if algorithim should be chosen automatically.",1,{"inputs":[{"name":"self"}],"output":{"name":"bool"}}],[11,"fmt","","",2,{"inputs":[{"name":"self"},{"name":"formatter"}],"output":{"name":"result"}}],[11,"clone","","",2,{"inputs":[{"name":"self"}],"output":{"name":"convbackwarddataalgo"}}],[11,"is_auto","","Check if algorithim should be chosen automatically.",2,{"inputs":[{"name":"self"}],"output":{"name":"bool"}}],[0,"frameworks","","Provides the specific Framework implementations for the Library Operations.",null,null],[0,"native","coaster_nn::frameworks","Provides NN for a Native backend.",null,null],[0,"helper","coaster_nn::frameworks::native","Provides useful macros for easier NN implementation for native.",null,null],[3,"NormalizationConfig","coaster_nn::frameworks::native::helper","",null,null],[3,"PoolingConfig","","",null,null],[12,"window","","",3,null],[12,"padding","","",3,null],[12,"stride","","",3,null],[3,"DropoutConfig","","",null,null],[12,"probability","","",4,null],[12,"seed","","",4,null],[3,"ConvolutionConfig","","",null,null],[12,"filter_shape","","",5,null],[12,"stride","","",5,null],[12,"padding","","",5,null],[5,"write_to_memory","","Just a helper function until SharedTensor has a nice interface for writing data",null,{"inputs":[{"name":"flatbox"},{"name":"t"}],"output":null}],[5,"sigmoid","","Computes the Sigmoid Function on the CPU",null,{"inputs":[{"name":"t"}],"output":{"name":"t"}}],[5,"sigmoid_grad","","Computes the Sigmoid Gradient on the CPU",null,{"inputs":[{"name":"t"},{"name":"t"}],"output":{"name":"t"}}],[5,"relu","","Computes the ReLU Function on the CPU",null,{"inputs":[{"name":"t"}],"output":{"name":"t"}}],[5,"relu_grad","","Computes the ReLU Gradient on the CPU",null,{"inputs":[{"name":"t"},{"name":"t"}],"output":{"name":"t"}}],[5,"tanh","","Computes the Tanh Function on the CPU",null,{"inputs":[{"name":"t"}],"output":{"name":"t"}}],[5,"tanh_grad","","Computes the Tanh Gradient on the CPU",null,{"inputs":[{"name":"t"},{"name":"t"}],"output":{"name":"t"}}],[11,"fmt","","",6,{"inputs":[{"name":"self"},{"name":"formatter"}],"output":{"name":"result"}}],[11,"clone","","",6,{"inputs":[{"name":"self"}],"output":{"name":"normalizationconfig"}}],[11,"fmt","","",3,{"inputs":[{"name":"self"},{"name":"formatter"}],"output":{"name":"result"}}],[11,"clone","","",3,{"inputs":[{"name":"self"}],"output":{"name":"poolingconfig"}}],[11,"fmt","","",4,{"inputs":[{"name":"self"},{"name":"formatter"}],"output":{"name":"result"}}],[11,"clone","","",4,{"inputs":[{"name":"self"}],"output":{"name":"dropoutconfig"}}],[11,"fmt","","",5,{"inputs":[{"name":"self"},{"name":"formatter"}],"output":{"name":"result"}}],[11,"clone","","",5,{"inputs":[{"name":"self"}],"output":{"name":"convolutionconfig"}}],[0,"cuda","coaster_nn::frameworks","Provides NN for a CUDA backend.",null,null],[8,"DataTypeInfo","coaster_nn::frameworks::cuda","CuDnn type info for generic use.",null,null],[10,"cudnn_data_type","","Mostly internal.",7,{"inputs":[],"output":{"name":"datatype"}}],[0,"helper","","Provides useful macros for easier NN implementation for CUDA/cuDNN.",null,null],[8,"ICudnnDesc","","",null,null],[10,"cudnn_tensor_desc","","",8,{"inputs":[{"name":"self"}],"output":{"name":"result"}}],[10,"cudnn_tensor_desc_softmax","","Creates a TensorDescriptor similar to `cudnn_tensor_desc`, but will create a fitting 4D tensor if the actual tensor would be 1D-3D.",8,{"inputs":[{"name":"self"}],"output":{"name":"result"}}],[10,"cudnn_tensor_desc_flat","","Creates a TensorDescriptor similar to `cudnn_tensor_desc`, but will create a fitting 3D tensor if the actual tensor would be 1D/2D.",8,{"inputs":[{"name":"self"}],"output":{"name":"result"}}],[10,"cudnn_filter_desc","","",8,{"inputs":[{"name":"self"}],"output":{"name":"result"}}],[10,"cudnn_convolution_desc","","",8,{"inputs":[{"name":"self"},{"name":"sharedtensor"}],"output":{"name":"result"}}],[8,"NNOperationConfig","coaster_nn","Provides generic NN Operation Config functionality.",null,null],[8,"ConvolutionConfig","","Provides Convolution Config functionality.",null,null],[11,"workspace_size","","Returns the largest workspace size in bytes needed for any of the convolution operations.",9,{"inputs":[{"name":"self"}],"output":{"name":"usize"}}],[8,"NN","","Provides the functionality for a backend to support Neural Network related operations.",null,null],[16,"CC","","The Convolution Operation Config representation for this Plugin.",10,null],[16,"CLRN","","The LRN Operation Config representation for this Plugin.",10,null],[16,"CPOOL","","The Pooling Operation Config representation for this Plugin.",10,null],[16,"CDROP","","The Dropout Operation Config representation for this Plugin.",10,null],[10,"init_nn","","Initializes the Plugin.",10,{"inputs":[],"output":null}],[8,"Sigmoid","","Provides the functionality for a Backend to support Sigmoid operations.",null,null],[10,"sigmoid","","Computes the [Sigmoid function][sigmoid] over the input Tensor `x`. [sigmoid]: https://en.wikipedia.org/wiki/Sigmoid_function",11,{"inputs":[{"name":"self"},{"name":"sharedtensor"},{"name":"sharedtensor"}],"output":{"name":"result"}}],[10,"sigmoid_grad","","Computes the gradient of a [Sigmoid function][sigmoid] over the input Tensor `x`. [sigmoid]: https://en.wikipedia.org/wiki/Sigmoid_function",11,{"inputs":[{"name":"self"},{"name":"sharedtensor"},{"name":"sharedtensor"},{"name":"sharedtensor"},{"name":"sharedtensor"}],"output":{"name":"result"}}],[8,"SigmoidPointwise","","Provides the functionality for pointwise Sigmoid operations (overwrites the input with the result of the operation).",null,null],[10,"sigmoid_pointwise","","Computes the [Sigmoid function][sigmoid] over the input Tensor `x`. [sigmoid]: https://en.wikipedia.org/wiki/Sigmoid_function",12,{"inputs":[{"name":"self"},{"name":"sharedtensor"}],"output":{"name":"result"}}],[10,"sigmoid_pointwise_grad","","Computes the gradient of a [Sigmoid function][sigmoid] over the input Tensor `x`. [sigmoid]: https://en.wikipedia.org/wiki/Sigmoid_function",12,{"inputs":[{"name":"self"},{"name":"sharedtensor"},{"name":"sharedtensor"}],"output":{"name":"result"}}],[8,"Relu","","Provides the functionality for a Backend to support ReLU operations.",null,null],[10,"relu","","Computes the [Rectified linear units][relu] over the input Tensor `x`. [relu]: https://en.wikipedia.org/wiki/Rectifier_(neural_networks)",13,{"inputs":[{"name":"self"},{"name":"sharedtensor"},{"name":"sharedtensor"}],"output":{"name":"result"}}],[10,"relu_grad","","Computes the gradient of [ReLU][relu] over the input Tensor `x`. [relu]: https://en.wikipedia.org/wiki/Rectifier_(neural_networks)",13,{"inputs":[{"name":"self"},{"name":"sharedtensor"},{"name":"sharedtensor"},{"name":"sharedtensor"},{"name":"sharedtensor"}],"output":{"name":"result"}}],[8,"ReluPointwise","","Provides the functionality for pointwise ReLU operations (overwrites the input with the result of the operation).",null,null],[10,"relu_pointwise","","Computes the [Rectified linear units][relu] over the input Tensor `x`. [relu]: https://en.wikipedia.org/wiki/Rectifier_(neural_networks)",14,{"inputs":[{"name":"self"},{"name":"sharedtensor"}],"output":{"name":"result"}}],[10,"relu_pointwise_grad","","Computes the gradient of [ReLU][relu] over the input Tensor `x`. [relu]: https://en.wikipedia.org/wiki/Rectifier_(neural_networks)",14,{"inputs":[{"name":"self"},{"name":"sharedtensor"},{"name":"sharedtensor"}],"output":{"name":"result"}}],[8,"Tanh","","Provides the functionality for a Backend to support TanH operations.",null,null],[10,"tanh","","Computes the [hyperbolic Tangent][tanh] over the input Tensor `x`. [tanh]: https://en.wikipedia.org/wiki/Hyperbolic_function",15,{"inputs":[{"name":"self"},{"name":"sharedtensor"},{"name":"sharedtensor"}],"output":{"name":"result"}}],[10,"tanh_grad","","Computes the gradient of [hyperbolic Tangent][tanh] over the input Tensor `x`. [tanh]: https://en.wikipedia.org/wiki/Hyperbolic_function",15,{"inputs":[{"name":"self"},{"name":"sharedtensor"},{"name":"sharedtensor"},{"name":"sharedtensor"},{"name":"sharedtensor"}],"output":{"name":"result"}}],[8,"TanhPointwise","","Provides the functionality for pointwise ReLU operations (overwrites the input with the result of the operation).",null,null],[10,"tanh_pointwise","","Computes the [hyperbolic Tangent][tanh] over the input Tensor `x`. [tanh]: https://en.wikipedia.org/wiki/Hyperbolic_function",16,{"inputs":[{"name":"self"},{"name":"sharedtensor"}],"output":{"name":"result"}}],[10,"tanh_pointwise_grad","","Computes the gradient of [tanh][tanh] over the input Tensor `x`. [tanh]: https://en.wikipedia.org/wiki/Hyperbolic_function",16,{"inputs":[{"name":"self"},{"name":"sharedtensor"},{"name":"sharedtensor"}],"output":{"name":"result"}}],[8,"Convolution","","Provides the functionality for a Backend to support Convolution operations.",null,null],[10,"new_convolution_config","","Creates a new ConvolutionConfig, which needs to be passed to further convolution Operations.",17,null],[10,"convolution","","Computes a [CNN convolution][convolution] over the input Tensor `x`. [convolution]: https://en.wikipedia.org/wiki/Convolutional_neural_network",17,null],[10,"convolution_grad_filter","","Computes the gradient of a [CNN convolution][convolution] with respect to the filter. [convolution]: https://en.wikipedia.org/wiki/Convolutional_neural_network",17,null],[10,"convolution_grad_data","","Computes the gradient of a [CNN convolution][convolution] over the input Tensor `x` with respect to the data. [convolution]: https://en.wikipedia.org/wiki/Convolutional_neural_network",17,null],[8,"Softmax","","Provides the functionality for a Backend to support Softmax operations.",null,null],[10,"softmax","","Computes a [Softmax][softmax] over the input Tensor `x`. [softmax]: https://en.wikipedia.org/wiki/Softmax_function",18,{"inputs":[{"name":"self"},{"name":"sharedtensor"},{"name":"sharedtensor"}],"output":{"name":"result"}}],[10,"softmax_grad","","Computes the gradient of a [Softmax][softmax] over the input Tensor `x`. [softmax]: https://en.wikipedia.org/wiki/Softmax_function",18,{"inputs":[{"name":"self"},{"name":"sharedtensor"},{"name":"sharedtensor"},{"name":"sharedtensor"}],"output":{"name":"result"}}],[8,"LogSoftmax","","Provides the functionality for a Backend to support LogSoftmax operations.",null,null],[10,"log_softmax","","Computes a logarithmic softmax over the input Tensor `x`.",19,{"inputs":[{"name":"self"},{"name":"sharedtensor"},{"name":"sharedtensor"}],"output":{"name":"result"}}],[10,"log_softmax_grad","","Computes the gradient of a logarithmic softmax over the input Tensor `x`.",19,{"inputs":[{"name":"self"},{"name":"sharedtensor"},{"name":"sharedtensor"},{"name":"sharedtensor"}],"output":{"name":"result"}}],[8,"LRN","","Provides the functionality for a Backend to support Local Response Normalization operations.",null,null],[10,"new_lrn_config","","Creates a new (Local Response Normalization) LRNConfig, which needs to be passed to further LRN Operations.",20,{"inputs":[{"name":"self"},{"name":"u32"},{"name":"f64"},{"name":"f64"},{"name":"f64"}],"output":{"name":"result"}}],[10,"lrn","","Computes a [LRN][lrn] over the input Tensor `x`. [lrn]: https://en.wikipedia.org/wiki/lrnal_neural_network",20,null],[10,"lrn_grad","","Computes the gradient of a [LRN][lrn] over the input Tensor `x`. [lrn]: https://en.wikipedia.org/wiki/lrnal_neural_network",20,null],[8,"Pooling","","Provides the functionality for a Backend to support Pooling operations.",null,null],[10,"new_pooling_config","","Creates a new PoolingConfig, which needs to be passed to further pooling Operations.",21,null],[10,"pooling_max","","Computes non-linear down-sampling ([max Pooling][pooling]) over the input Tensor `x`. [pooling]: https://en.wikipedia.org/wiki/Convolutional_neural_network#Pooling_layer",21,null],[10,"pooling_max_grad","","Computes the gradient of [max Pooling][pooling] over the input Tensor `x`. [pooling]: https://en.wikipedia.org/wiki/Convolutional_neural_network#Pooling_layer",21,null],[10,"pooling_avg","","Computes non-linear down-sampling ([average Pooling][pooling]) over the input Tensor `x`. [pooling]: https://en.wikipedia.org/wiki/Convolutional_neural_network#Pooling_layer",21,null],[10,"pooling_avg_grad","","Computes the gradient of [average Pooling][pooling] over the input Tensor `x`. [pooling]: https://en.wikipedia.org/wiki/Convolutional_neural_network#Pooling_layer",21,null],[8,"Dropout","","Provides the functionality for a Backend to support Dropout operations.",null,null],[10,"new_dropout_config","","Creates a new DropoutConfig, which needs to be passed to further dropout Operations.",22,{"inputs":[{"name":"self"},{"name":"f32"},{"name":"u64"}],"output":{"name":"result"}}],[10,"dropout","","Computes non-linear down-sampling ([max Pooling][pooling]) over the input Tensor `x`. [pooling]: https://en.wikipedia.org/wiki/Convolutional_neural_network#Pooling_layer",22,null],[10,"dropout_grad","","Computes non-linear down-sampling ([max Pooling][pooling]) over the input Tensor `x`. [pooling]: https://en.wikipedia.org/wiki/Dropout_(neural_networks)",22,null],[14,"impl_ops_sigmoid_for","","",null,null],[14,"impl_ops_relu_for","","",null,null],[14,"impl_ops_tanh_for","","",null,null],[14,"impl_ops_softmax_for","","",null,null],[14,"impl_ops_log_softmax_for","","",null,null],[14,"impl_ops_lrn_for","","",null,null],[11,"workspace_size","","Returns the largest workspace size in bytes needed for any of the convolution operations.",9,{"inputs":[{"name":"self"}],"output":{"name":"usize"}}]],"paths":[[4,"ConvForwardAlgo"],[4,"ConvBackwardFilterAlgo"],[4,"ConvBackwardDataAlgo"],[3,"PoolingConfig"],[3,"DropoutConfig"],[3,"ConvolutionConfig"],[3,"NormalizationConfig"],[8,"DataTypeInfo"],[8,"ICudnnDesc"],[8,"ConvolutionConfig"],[8,"NN"],[8,"Sigmoid"],[8,"SigmoidPointwise"],[8,"Relu"],[8,"ReluPointwise"],[8,"Tanh"],[8,"TanhPointwise"],[8,"Convolution"],[8,"Softmax"],[8,"LogSoftmax"],[8,"LRN"],[8,"Pooling"],[8,"Dropout"]]};
initSearch(searchIndex);
